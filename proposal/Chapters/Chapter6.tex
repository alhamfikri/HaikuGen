\section{Evaluation}

We propose a Turing test to evaluate our system and the research outcome in general. In this test, we create several automated generated Haiku and several human created Haiku. Those Haiku will be mixed into a single set. Some respondents will then decide which one is computer generated and which one is created by real human. This test will observe how good our Haiku in terms of its similarity to real human creation. Another evaluation can be achieved by asking the respondents some question and score on various aspects of the Haiku, such as "Do you understand this Haiku?" or "How beautiful this Haiku". 

In engineering aspect, we could perform some test on how fast the system build the Haiku. Some further observation regarding the relation between score weighting and threshold towards the execution time can be performed to find out the best configuration so that the resulting Haiku is not rubbish, yet still takes reasonable amount of computational time. Another aspect that can be observed is the correlation between the resource used towards the generated Haiku. In this evaluation, we want to find out what is the best text resources for Haiku generation and automated poetry in general.